---
title: "Homework - Multivariate Analysis"
author: "Javier Esteban Aragoneses - Mauricio Marcos Fajgenbaun"
date: "12/12/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An analysis of Diabetes in Indian Women

## Motivation of Research and Dataset 

For this research, we selected a data set of the Diabetes. Diabetes is an illness that occurs when the sugar in blood (also called glucose) is too high. At the same time, insulin (produced by the pancreas), is produced to help the glucose make it to human cells. When the pancreas does not produce enough insulin or none at all, then this glucose remains in the blood causing serious health issues.

With the years, diabetes had become a larger problem, especially for western societies. Eating fast food or high-processed meals have led to enormous ingestions of sugar and, with this, an historical increase of diabetes in most countries of the world. 

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The main objective that explains the reason of the existence of this data set, is to diagnostically predict weather or not a patient has diabetes, based on certain measurements of several variables that are included in the dataset. At the same time, this database contains only data of women of at least 21 years old of Pima Indian heritage (north American indigenous women, that live in the State of Arizona and the Mexican states of Sonora and Chihuahua).  Diabetes is quite prevalent in this group of Native Americans living in those places in particular. 

So, we can say that the aim is to study the cause of diabetes in this particular ethnic group. This disease was very uncommon in Pima Indians until the second half of the 20th Century. Nevertheless, as King et al. informs in 1993, the highest prevalence of type 2 diabetes in the world was found in this particular ethnic group, as more than half of the women older than 35 years-old would suffer from this disease. 

So, in this research we will study how health predictors are associated with the presence of diabetes in Pima Indians. According to the World Health Organization criteria, if the 2-hour post-load plasma glucose was at least 200 mg/dl at any survey examination or if found during any moutine medical care. 

In this dataset, 768 women were registered in the database. From this total, a 35% (268 in total) had diabetes, while the rest (500: 65%) did not suffer from this disease. 
In the next table, we show the variables included in the data set with they specific classification. 

##  Variables Description

Pregnancy: quantitative - discrete
Glucose: quantitative - continuous
Blood Pressure: quantitative - continuous
Skin Thickness: quantitative - continuous
Insulin quantitative - continuous
BMI: quantitative - continuous
Diabetes Pedigree Function: quantitative - continuous
Age: quantitative - discrete
Outcome: qualitative- nominal

The variable “Outcome” is a categorical variable that accounts for weather a specific person is diabetic (takes value = 1) or not diabetic (takes value = 0). 

## Exploratory Analysis

First, let´s install all the packages we will use. Then, we will read the file and start working on it.

```{r}
library("tidyverse")
library("ggplot2")
library("dplyr")
library("moments")
library("gridExtra")
library("rrcov")
library("rpart.plot")
library("mice")
library("MASS")
library("andrews")
library("ggcorrplot")
library("FactoMineR")
library("factoextra")
library("paran")
library("corrplot")
library("corpcor")
library("RSpectra")
library("factoextra")
library("cluster")
library("ape")
library(mclust)



```

```{r}
ColClasses=c(rep("numeric",9))
DATOS=read.csv2("diabetes.csv",sep = ",",header = T, colClasses = ColClasses, dec = "." )
summary(DATOS)
```


When studying the variables, we can tell that there are some missing values in some of the variables. This will be a big burden later, when trying to study correlation between the variables and the real dimension of it. Also, it will blur our understanding of the data. Thus, we will impute the missing values first.

To do so, we change the values "0" from the second to the eight variable to "NA". We only make use of these 7 variables, because the first variable (pregnancy) can take the value "0" and our variable number 9 is "outcome" and its value is either "1" or "0". Of course, we wouldn´t want to change their zero values. Nevertheless, we know that as we are talking about humans (that are actually alive) it is impossible to get an observation of these variables equal to zero. Easy example: it is obvious nobody has a blood preassure equal to zero. This is why we consider values "0" for these variables as missing values.

We will impute the values with the library "MICE". 
```{r}
data <- DATOS[2:8]
data[data==0] <-  NA
DATOS[2:8] <- data
summary(DATOS[,1:8])
```
```{r}
sum(is.na(DATOS))
```

This is a big problem. We can see that we have a lot of missing value in our data (652 values are missing). 
When checking more closely, we can say that at least 30% of our rows are being affected by this problem. We certainly can not give up 30% of our data, as we would lose too much information. We have to impute this values using the package "Mice".

```{r}
DATOS <- mice(DATOS,m=1,method="pmm")
DATOS <- complete(DATOS)
attach(DATOS)
```
Now that we solved the problem of missing values, we will create a new variable called "Dataset.norm" to use later on, when we will need to standarize our variables for principal component analysis.
```{r}
Dataset.norm <- DATOS
# Unitization with zero minimum ((x-min)/range))
Dataset.norm[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
```

Let´s first dig in into our categorical variable. 

```{r}
ggplot(data=DATOS, aes(x = Outcome)) + geom_bar(aes(fill = Outcome)) + ggtitle("Bar Plot por Outcome")
frec_cal <- table(DATOS$Outcome)
frec_cal
tabl_cont <- prop.table(table(DATOS$Outcome))
tabl_cont
```

As we can see, 65% of our sample does not suffer from diabetes, while 35% does suffer from diabetes. This is a huge proportion, given that this sample is taken from one population in particular (it is certanily a population with a huge incidence of diabetes). One every three persons in this sample has diabetes.

Let´s check the behaviour of our quantitative variables by themselves first. Now, we will check their histograms and boxplots.

### Histograms per variable

```{r}
hist(Age)
hist(Pregnancies)
hist(Glucose)
hist(BloodPressure)
hist(SkinThickness)
hist(Insulin)
hist(BMI)
hist(DiabetesPedigreeFunction)
```
### Boxplot per variable

```{r}
par(mfrow=c(1,1))
boxplot(Age,main ="Age")
boxplot(Pregnancies, main="Pregnancies")
boxplot(Glucose,main="Glucose")
boxplot(BloodPressure,main="Blood Pressure")
boxplot(SkinThickness, main = "Skin Thickness")
boxplot(Insulin, main = "Insulin")
boxplot(BMI, main = "BMI")
boxplot(DiabetesPedigreeFunction, main="Diabetes Pedigree Function")
```

Let´s now plot the kernel for each variable by itself, and then each kernel by subgroups: diabetic and non-diabetic.
```{r}
dat <- DATOS
dat$Outcome <- as.factor(dat$Outcome)
```

```{r}
univar_graph <- function(univar_name, univar, DATOS, output_var) {
  g_1 <- ggplot(DATOS, aes(x=univar)) + geom_density() + xlab(univar_name)
  g_2 <- ggplot(DATOS, aes(x=univar, fill=output_var)) + geom_density(alpha=0.4) + xlab(univar_name)
  grid.arrange(g_1, g_2, ncol=2, top=paste(univar_name,"variable", "/ [ Skew:",skewness(univar),"]"))
}

for (x in 1:(ncol(DATOS)-1)) {
  univar_graph(names(DATOS)[x], DATOS[,x], DATOS, dat[,'Outcome'])
}

```


```{r}
par(mfrow=c(1,1))

boxplot(Age~Outcome,ylab="Outcome",xlab="Age",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Pregnancies~Outcome,ylab="Outcome",xlab="Pregnancies",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Glucose~Outcome,ylab="Outcome",xlab="Glucose",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(BloodPressure~Outcome,ylab="Outcome",xlab="BloodPresure",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(SkinThickness~Outcome,ylab="Outcome",xlab="SkinThickness",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Insulin~Outcome,ylab="Outcome",xlab="Insulin",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(BMI~Outcome,ylab="Outcome",xlab="BMI",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(DiabetesPedigreeFunction~Outcome,ylab="Outcome",xlab="Diabetes Green function",col=c("lightblue","orange"),horizontal=TRUE)
```
Let´s comment on the plots done above.

### Age
This variable account for the age of each women in the sample. It varies from a minimum of 21 to a maximum of 81. This means that were included women in that age frame. As we can see in the plot below, the median is on 29, meaning that half of the women are more that 29 years old and the other half is older than 29. As the first quartile is on 24 and the third is on 41, half of the women from this sample is between those ages (24 and 41). This shows us that the population is relatively young, although there are 25% of women in the sample that are from 41 to 81 years old. 

At the same time, we can see that when comparing by outcome (if they resulted ill or not) the median of the diabetic ones is higher than the non-diabetic ones. The distribution of this variable looks more left-skewed for the non-diabetic women, in comparison with the ones that are diabetic. This, all together, may mean that inside the non-diabetic woman, most of them are pretty young, and some of them are old. The same happens with the diabetic women: half of them are younger than 35, and the rest are older. 

### Blood Preassure
When studying blood pressure, we must first say that it is measured in mm/gg. The median is around 70, and the distribution looks a bit symmetrical. When dividing the observations between diabetic and not diabetic, we can say that both distributions look pretty similar.  

### BMI
The body mass index is a very useful index to take into account the weight, but related to the height of the person. This way, it is measured: weight (in Kg) / [height (in meters)] ^2. 
As we can see both in the histogram and in the boxplot, the distribution seems symmetrical and the size of the box is quite small (so half of the women of the sample, between the first and third quantile, are concentrated in the values 27.30 and 36.60. Normally having a value between 25 and 30 is considered right, while more than 30 is considered overweight, and more than 35 obesity. 


### The  Diabetes Pedigree Function
The diabetes Pedigree Function scores the likelihood of diabetes based on family history. So the larger value this variable takes, the more likely to be diabetic. As we can see in the plots, our distribution in this case seems right skewed. 
At the same time, when comparing between groups (diabetic and not diabetic) the distributions look  similar, although the third quartile for the diabetic group goes further than the not diabetic one. At the same time, the 4th quartile looks bigger in the case of the not diabetic group


### Glucose
Glucose measures the plasma glucose concentration over 2 hours in an oral glucose tolerance test. As it can be seen, the median is 117 and is very close to the mean (120.9) meaning that it is a bit symmetrical. At the same time, there is some skewedness, as the tail on the right looks much heavier. When classifying between diabetic or not, the median of level of plasma glucose in women with diabetes is higher than the ones that have no diabetes. At the same time, the diabetic women present a wider range of glucose values between the 1st. and 3rd. quartile. 


### Insulin
Insulin is measured in mu U/ml and its distribution looks right-skewed. When looking at the histograms, most of the values are situated in the left side of the distribution, meaning that most women of the sample have low insulin level. A “standard” or “normal” level for this 2 hour test is considered at around 16 to 166 U/ml and the median of our variable in the sample is 30.5 (not very high). When dividing it between diabetic or not, both distributions look similar (with the same long tail) and the biggest interval of frequency at the lowest level of the distribution.  


### Pregnancies
This variable account for the number of pregnancies that each women had during their life. The minimum in the sample is 0 pregnancies and the maximum is 17 pregnancies. The distribution has a heavier tail on the left, meaning it is right-skewed. This is coherent with the fact that most of the woman from the sample are relatively young, and thus did not have so many kids yet. This explains the fact that the median is at 3, meaning that half of the women have less than 3 kids while the rest account for more than 3. Then, the third quartile is at 6, meaning that only 25% of the women fron the sample have between 6 and 17 kids. 
When studying this variable by group of diabetic and not diabetic, we can say that the median of the diabetic women is higher than the one of not diabetic.  


### Skin Thickness
The skin thickness measures the triceps skin fold thickness in mm. As it can be seen in the plots, the distribution looks pretty right skewed, meaning that most of the observation occur in the smallest values of the variable. The median is at 23, meaning 50% of the women have less than 23mm of skin thickness while the other half have more. At the same time, when dividing between groups of diabetic or not, both distribution look similar, although the diabetic looks less dispersed and its median is larger than the ones of non-diabetics. 

In general, there are some variables that are highly positively skewed (Insulin, DiabetesPedigreeFunction, Age) while others are highly negative skewed like BloodPressure.

### Multivariate Analysis

We now perform a matrix of plots, showing possible correlation between different variables (every variable against every variable). We can not say we see a clear pattern of linear correlation between variables. At least, not yet.

```{r}
X_quan <- DATOS[,1:8]
pairs(X_quan,pch=20,col="lightblue")
colors_Student <- c("lightblue","orange")[1*(Outcome=="1")+1]
pairs(X_quan,pch=19,col=colors_Student)
```

Then, we perform a PCP. The Parallel Coordinates Plot is very useful to visualize and analyze high-dimensional data. 

```{r}
parcoord(X_quan,col="lightblue",var.label=TRUE,main="PCP for diabetics")
parcoord(X_quan,col=colors_Student,var.label=TRUE,main="PCP for diabetes in terms of Outcome")
```

As we might expect, being diabetic is directly related to glucose level. There is also a lesser correlation between the other variables and diabetes, except in cases of pregnancy and diabetes pedigree function in which there is no relationship. 

```{r}
par(mfrow=c(1,1))
andrews(as.data.frame(cbind(X_quan,as.factor(DATOS[,9]))),clr=8,ymax=4,main="Andrews' Plot for women in terms of Diabetic or not")
```

# Outliers Detection

In order to study the mean vector, covariance and correlation matrix, we need to take into account outliers, as these measures tend not to be robust. This means that the presence of outliers may change some of our conclusions. This is why we will do this treatment now.

In order to be able to use MCD to treat outliers, we must do some preprocessing of our data.
This is because we need our variables to behave as Gaussian variables (symmetric at least). To achieve this, we perform logarithmic transformations when required. Let´s plot histograms for each variable and take a look at each of their behaviour. It is important to check for outlyers because they may affect our interpretatin of the data.

After looking at the histograms, we can tell that variables: "age", "pregnancies", "skinthickness", "insulin" need an appropiate transformation. 

```{r}
# "Ages" transformation
Ages= log(Age)
hist(Ages)
DATOS$Age <- Ages

# We standarize the variable Pregnancies
hist(DATOS$Pregnancies)
DATOS$Pregnancies=scale(Pregnancies, center = TRUE,scale= TRUE)
hist(DATOS$Pregnancies)

```

```{r}
# We normalize
DATOS$SkinThickness <- scale(log(DATOS$SkinThickness))

# We normalize
hist(log(Insulin))
DATOS$Insulin <- log(DATOS$Insulin)
attach(DATOS)
```


Now that we finished with our transformations, we proceed on applying the MCD.
```{r}
X=DATOS[1:8]
n <- nrow(X)
p <- ncol(X)
color_1 <- "deepskyblue2"
color_2 <- "seagreen2"
color_3 <- "orange2"
MCD_est <- CovMcd(X,alpha=0.75,nsamp="deterministic")
m_MCD <- MCD_est$center
```
Let´s now check on the mean vector, covariance and correlation matrix of the variables before taking the outliers out.


Let´s check first the mean vector, correlation and covariance matrix before taking out outliers.
```{r}
m <- colMeans(DATOS[,1:8])
m
S <- cov(X)
S
R <- cor(X)
R
ggcorrplot(R)
```

## Outliers Treatment

We will find the rows with observations that can be considered outliers.
```{r}
#Let´s check on the outliers

X_sq_Mah_MCD <- MCD_est$mah

col_outliers_Mah_MCD <- rep(color_2,n)
outliers_Mah_MCD <- which(X_sq_Mah_MCD>qchisq(.99,p))
outliers_Mah_MCD
```

 We can plot and check for these outliers aboev the Mahalanobis Distance. 
```{r}
col_outliers_Mah_MCD[outliers_Mah_MCD] <- color_3
par(mfrow=c(1,2))
plot(1:n,X_sq_Mah_MCD,pch=19,col=col_outliers_Mah_MCD,main="Squared Mahalanobis distances",xlab="Observation",ylab="Squared Mahalanobis distance")
abline(h=qchisq(.99,p),lwd=3,col="red")
plot(1:n,log(X_sq_Mah_MCD),pch=19,col=col_outliers_Mah_MCD,main="Log of squared Mahalanobis distances",xlab="Observation",ylab="Log of squared Mahalanobis distance")
abline(h=log(qchisq(.99,p)),lwd=3,col="red")
```
Now, we see the observations bellow and above the log of squared Mahalanobis distance. This is a very good way of observing our observations that fall very far away from the centroid of the data. We can also see the outliers in a scatterplot.


We can plot a Scatterplot showing the outliers. 
```{r}
pairs(X,pch=19,col=col_outliers_Mah_MCD)


attach(DATOS)
```

It is easy to see that there are some observations 
Now, we can substract these rows of observations, in order to be able to understand the mean, and correlations with a better view.

```{r}
C=DATOS[-c(5  , 9  ,13  ,14  ,19  ,40,  44,  46,  59,  68 ,107 ,121 ,126 ,130 ,160 ,178 ,188,194,222, 229, 246 ,255 ,295 ,304 ,324 ,331 ,371 ,372 ,396 ,409,435,446 ,454, 457, 465 ,488,520 ,538 , 550, 580 ,594 ,597, 598 ,622 ,623 ,662 ,667 ,674 ,692 ,703 ,745),]

DATOS <- C
```
Even taking out the outliers, these following variables don´t look so gaussian
```{r}
hist(C$Pregnancies)
hist(C$Age)
```
As we have two subgroups inside of our data, people with diabetes and people not suffering from this dessease (and our objective is obviously related with understanding how this illness behaves among this specific population) we will perform this study for both subgroups and for the total, and then compare them.
```{r}
cd <- C[ C$Outcome==1,]
cd <- cd[,1:8]
cnd <- C[ C$Outcome==0,]
cnd <- cnd[,1:8]
attach(DATOS)
```

First, let´s study this three things for the entire group. 

```{r}
# For the sample totallity
# Mean Vector
m_MCD
```

```{r}
# Covariance Matrix
S_MCD <- MCD_est$cov
S_MCD
```

```{r}
# Correlation Matrix
R_MCD <- cov2cor(S_MCD)
R_MCD
```
Let´s plot this correlation matrix.

```{r}
ggcorrplot(R_MCD)

```
As we can see in this plot, there are some interesting linear relationships between some variables. First, let us say there is no negative linear relationship between any variable. We only have positive correlations, as the negative ones are very close to zero.

They are detailed as follows:

a) there is a positive linear relationship that may be significant between "age" and "pregnancy" meaning that the older people gets they have more pregnancies. This is very trivial: the chance of having had more kids obviosuly increases when the person is older, as pregnancies only increase with age, or stay the same (it is the total amount of pregnancies, a cummulative quantity).
b) Both "insulin" and "BMI" are positively correlated with "glucose". This does not surprise us either. People with higher body mass index have more chances of having high levels of sugar in blood. At the same time, people with high insulin levels after applying the medical test and check their insulin response, tend to have higher levels of glucose.
c) "BMI" and "SkinThickness" also appear to have a significant linear relationship between them. This may tell us thay people with higher body mass index tend to have higher values for skinthikness (and the other way around). This is also reasonable.
d) "Age" looks positively correlated to every other variable measured, with exception of the pedigree diabetes function. The more aged the person, the higher their values of insulin, blood preassure, skinthickness, glucose, BMI. 

Let´s check what happens when we include the variable "Output" in our correlation matrix.

```{r}
Y <- DATOS [,9]
Mat_cov <- cov(DATOS)
mat_cor <- cov2cor(Mat_cov)
mat_cor
```
As we can see, the only value that calls our attention is the correlation between "Outcome" and "Glucose", for being far away from 0. If the correlation between a categorical and quantitative variable is close to 1, it means that subjects with outcome = 1 (diabetics) have larger values than subjects with outcome = 0 (non-diabetic). This basically says that in general the diabetic subset of the sample will have a larger value of glucose in blood. Not a big surprise.


Now, first for the diabetic subgroup (outcome == 1).

```{r}
# Mean Vector
apply(cd,2,mean)
# Covariance Matrix
cov(cd)
l=cov(cd)
# Correlation Matrix
cov2cor(l)
```
Same procedure, for the non-diabetic subgroup (outcome == 0).

```{r}
# Mean Vector
apply(cnd,2,mean)
```

```{r}
# Covariance Matrix
cov(cnd)
n=cov(cnd)
```

```{r}
# Correlation Matrix
cov2cor(n)
```

Women with diabetes seem to have more pregnancies, have in average higher glucose, blood preassure, skin thickness, BMI. It also seems that there is a positive correlation between age and diabetes (older people tend to get it more) and the variance of the glucose is much larger in non-diabetic people.

# Let´s plot a scatterplot without outliers

```{r}
d=C[,1:8]
pairs(d,pch=19,col=colors_Student)
```

#Principal Component Analysis

As the dimensionalities of the data grows, the feature space grws rapidly. We care about this for several reasons: first, we want to minimize computational cost. Also, when the dimensionality is big, the data starts getting more and more difficult to interpret, and it gets more difficult to detect underlying patterns.

When having so many features, we fail to see the "real" or "intrisic" dimensionality of the data. In other words, the dimensions could probably be reduced to a smaller set of dimensions.

```{r}
dim(DATOS)
head(DATOS)
```

First I define a variable Y, where my categorical variable will be saved. This is due to the fact that we need quantitative variables to perform Principal Component Analysis.
I already defined X_quan as the quantitative variables of my model.

Then, I will define de number or rows and columns as "n" and "p" respectively.

```{r}

X <- Dataset.norm[1:8]

n <- nrow(X)
n
p <- ncol(X)
p
```
We have a matrix with dimension 768 x 8.
The relationship between principal components and covariance matrix is fundamental to understand. First, the correlation matrix will help us observe if there are variables that are linearly related, and thus may be redundant when trying to explain or describe a sample.

As we can see, sometimes the size of the correlation matrix may blure our interpretations of linear correlations. 

It is still hard to trace patterns, by only basing ourselves on the correlation structure of the data. As we are not endocrinologist, who have a lot of knowledge (not even close) of the domain we are studying, we will not go forward in taking a variable out of the study by simple choice. By doing this, we may be omitting something important in our data.
This is why we will use PCA. With PCA, we will use a dimensionality reduction technique to remove the redundancy introduced by highly correlated variables.

What does PCA basically do? 
a) Removes noise from correlations
b) changes de original coordinates, and stablishes new ones (principal components)
c) Finally, helps us decide what is the real dimensionality of the data (how components are we finally considering)

Principal components are new variables that are built from linear relationships from the original variables. This new variables are uncorrelated to each other, and they try to compress the most information possible in the firsts components.

For this, we will use the previously determined normalized variables. They are also standarized, so that we won´t have any issues with scale differences.By only checking the values, we can say that the variables had also been standarized, so there is no scaling problem. This is fundamental to get a valuable insight of the true dimensionality of the data, as if there variables meassured in very different scales, one will overweight the other, hiding valuable information.
```{r}
head(X)
```
We perform the PCA, and proceed to check the first 10 elements with summary.
```{r}
pca_output <- PCA(X,ncp = 8, graph = FALSE)
summary(pca_output, nbelements = 10)
```
We have as many eigenvectors and eigenvalues as variables in our numerical matrix. But the main idea of this analysis, is to be able to compress the information in less variables. It would not be of any use to take the 8 principal components (as there is no direct interpretation). In other words, if we would stick with eight variables, we would just keep on working with the original ones. 

The principal components do have a very interesting geometrical interpretation. This is because the principal components represent the direction of the data that explain a maximum amount of variance. So they act like new axes, that provide the best "angle" to analyze the data. 

Let´s try and decide how many principal components we will take.

a) Method Nº1
The screeplot is a barplot of the proportion of explained variance of each dimension.
```{r}
fviz_screeplot(pca_output,ncp=8,addlabels=T,barfill="blue",barcolor="red")
```
According to this rule, we could make use of the first 5 principal components (or only with the first two).
Checking the Kaisser-Gutman Rule, we can check which PC has eigenvalue larger than 1.
```{r}
pca_output$eig
```

According to this test, we should stick to the first three principal components, and maybe the fourth (as its eigenvalue is very close to 1).
This is because components with eigenvalue less than 1, explain less of the total variability that an original variable does, on average.
Therefore, by choosing principal components with eigenvalues greater than one, we maintain those that express more of the variability than each of the original variables.

We can also try with Parallel Analysis technique, using the package "paran".
```{r}
pca_output_ret <- paran(X,seed=1,graph = TRUE)
pca_output_ret$Retained
```

In this case, the papallel Analysis tells us that we should stay with the first three principal components, as they have an adjusted eigenvalue larger than 1. But first, we can take a look at the contribution of each variable in the construction of each principal component.
```{r}
pca_output$var$contrib
```

Get the variance of the 3 first dimensions
```{r}
pca_output$eig[,2][1:3]
```
Get the cummulative variance of the 3 first dimensions
```{r}
pca_output$eig[,3][1:3]
```

We can say that sticking with only 3  dimensions (reducing our dimensionality by more than 50%) we could explain more than 65% of the information from our dataset. 

Interpreting the contribution of each variable to the principal component can be a bit unclear by only checking the numbers. Nevertheless, we can use some graphical images to have a better understanding.
```{r}
fviz_pca_var(pca_output,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

Creating a factor Map for variables with COS2 higher than 0.7. COS2 is also a useful measure that expresses
```{r}
fviz_pca_var(pca_output,select.var=list(cos2=0.6),repel=TRUE)
```

Checking the contribution of the most influential 5 variables on the first three principal components.
```{r}
fviz_contrib(pca_output, choice="var", axes=1,top=5)
fviz_contrib(pca_output, choice="var", axes=2,top=5)
fviz_contrib(pca_output, choice="var", axes=3,top=5)
```

Obtaining a barplot for the variables with highest COS2 in the 1st, 2nd and 3rd principal component. The squared cosine shows the importance of a component for a given observation.

```{r}
fviz_cos2(pca_output, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output, choice = "var", axes = 3, top = 5)
```
We can also plot the correlation matrix of the components with the original variables. This is important, because it can show us the important variables of the original data in terms of variability.

```{r}
X_pcs <- prcomp(X,scale=TRUE)
corrplot(cor(X,X_pcs$x[,1:4]),is.corr=T)
```


By checking the principal component analysis, we can´t say that any of them really help us to understand the classification of observations between diabetic and non-diabetic. Nevertheless, it can give us a hint to what variables we can get.
As far as each component is considered:
a) Given that the variables that contribuite the most to the first principal components are all related to health, we can say that it makes a ranking in which it ranks how healthy a person is (each observation). As it is a combination of the glucose level, insuline, BMI and Skin Thickness.
b) The second principal component seems to rank observations according to some descriptive characteristics related to life cycle or maybe fertility (specially number of pregnancies, and age). It is reasonable that both variables contribute to this component as they are linearly correlated, as seen before.

We can observe the correlation between a component and a variable (thus, the information they share). This is called "loading".

```{r}
# Check if the first two components give me any sign of subgroups.
colors_X <- c(color_2,color_3)[1*(Y=="1")+1]
par(mfrow=c(1,1))
pairs(X_pcs$x[,1:3],col=colors_X,pch=19,main="The first three PCs")
```
It seems that the principal components don´t help us a lot in dividing the observations between diabetic and non-diabetic.

```{r}
summary(dat$Outcome)
fviz_pca_ind(pca_output,label="var",habillage=dat$Outcome,addEllipses=TRUE)
```

We get reassured: the first two principal components don´t seem to give us enough information to determine the subgroups: diabetic and non-diabetic. But we can still see the first principal component may be an indicator of illness (but both groups share a big surface). 

Now, let´s perform the subgroup analysis.

# PCA for both subgroups


First, I divide the sample in both subsets and normalize (and standarize) all the variables inside of each subset.

```{r,echo=FALSE}
Dataset.norm_diab <- Dataset.norm[Dataset.norm$Outcome==1,]
Dataset.norm_nodiab <- Dataset.norm[Dataset.norm$Outcome==0,]

Dataset.norm_diab[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
X_diab <- Dataset.norm_diab[1:8] 

Dataset.norm_nodiab[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
X_nodiab <- Dataset.norm_nodiab[1:8] 

```

## PCA for diabetic subgroup (outcome == 1)

```{r}
pca_output_diab <- PCA(X_diab,ncp = 8, graph = FALSE)
summary(pca_output_diab, nbelements = 10)
```
We now check how many components we will use for our analysis.
```{r}
# Method 1
fviz_screeplot(pca_output_diab,ncp=8,addlabels=T,barfill="blue",barcolor="red")

# Method 2
pca_output_diab$eig

# Method 3
pca_output_ret <- paran(X_diab,seed=1,graph = TRUE)
pca_output_ret$Retained
```
We also keep 3 principal components in this case.

Let´s see how much variability of the data they can explain.
```{r}
pca_output_diab$eig[,2][1:3]
```
```{r}
pca_output_diab$eig[,3][1:3]
```
We can explain almost 70% of the information of the data by only taking three variables. This is how powerful this method is: we can only make use of less than half of our variables and explain almost 70% of the variability. Of course, if we would have a larger number of variables in our original data we could see this method much more useful.

Let´s check the contributions of each variable.
```{r}
pca_output_diab$var$contrib
```

```{r}
fviz_pca_var(pca_output_diab,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

```{r}
fviz_pca_var(pca_output_diab,select.var=list(cos2=0.6),repel=TRUE)
```
Just as before, all the original variables are positively correlated with the first principal component, but not all of them are positively correlated with the second principal component. Between the variables that have more than 0.6 as COS2, we get "age", "pregnancies" and "BMI". We can see that both age and pregnancies are negatively correlated to the second component, while the BMI is positively correlated to both first and second component.

```{r}
fviz_contrib(pca_output_diab, choice="var", axes=1,top=5)
fviz_contrib(pca_output_diab, choice="var", axes=2,top=5)
fviz_contrib(pca_output_diab, choice="var", axes=3,top=5)
```

```{r}
fviz_cos2(pca_output_diab, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output_diab, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output_diab, choice = "var", axes = 3, top = 5)
```
It seems that the first principal component encapsulates a lot of information regarding specifical issues of health. It calls our attention that the variable that contributes more to the third principal component is the Diabetes Pedigree Function. This is an important point, as the Diabetes Pedigree Function talks about the inheritence of the desease. It shows us, somehow, that diabetes may follow a genetic pattern, so those whose ancestors suffered from diabetes may get diabetes too (as now we are only studying people with diabetes). 

Now, let´s do the same with subgroup of non-diabetic.

```{r}
pca_output_no_diab <- PCA(X_nodiab,ncp = 8, graph = FALSE)
summary(pca_output_no_diab, nbelements = 10)
```
Let´s check how many principal components we are taking for our analysis.
```{r}
# Method 1
fviz_screeplot(pca_output_no_diab,ncp=8,addlabels=T,barfill="blue",barcolor="red")

# Method 2
pca_output_no_diab$eig

# Method 3
pca_output_ret <- paran(X_nodiab,seed=1,graph = TRUE)
pca_output_ret$Retained
```
And again, we will take just 3. Let´s check contributions.

```{r}
pca_output_no_diab$var$contrib
```

```{r}
fviz_pca_var(pca_output_no_diab,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

```{r}
fviz_pca_var(pca_output_no_diab,select.var=list(cos2=0.6),repel=TRUE)
```


```{r}
fviz_contrib(pca_output_no_diab, choice="var", axes=1,top=5)
fviz_contrib(pca_output_no_diab, choice="var", axes=2,top=5)
fviz_contrib(pca_output_no_diab, choice="var", axes=3,top=5)
```
```{r}
fviz_cos2(pca_output_no_diab, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output_no_diab, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output_no_diab, choice = "var", axes = 3, top = 5)
```

Now we can see that the new variables behave similar to the ones in the whole data. The first component takes us to a ranking of health, the second one talks about cycle of life or fertility. Now the pedigree function don´t seem to have a big influence on any of the principal components.

Let´s compare the correlation matrix of both subgroups (their principal components and original variables).

```{r}
X_pc_diab <- prcomp(X_diab,scale=TRUE)
corrplot(cor(X_diab,X_pc_diab$x[,1:3]),is.corr=T)

X_pc_no_diab <- prcomp(X_nodiab, scale=TRUE)
corrplot(cor(X_nodiab,X_pc_no_diab$x[,1:3]), is.corr=T)
````
We can see that for bouth groups, both the first and second principal component look similar in regards to their relationship with the original variables. The one that changes a bit is the third one. This analysis may be telling us that our variables do not help us a lot in distinguishing diabetic from non-diabetic. But we would have to perform clustering techniques to check this more in depht.

# Second part

In this part we will continue with the clustering classification. This is a kind of classification is in the groups of unsupervised classification because we dont need a previous knowledge  about the groups that exists.In supervised classification we need to train the data knowing that each observation belongs to a group. Here. We create the groups and we put each observation in each group. 

Firsts we start with partitional clustering. We start from an initial cluster definition and then we proceed by exchanging elements between cluster until an appropriate cluster structure is found. In order to do that , we have to select the number of clustering. For it, we will check the optimal number of clusters with three criteria; WSS, average silhouette and gap statistic.
```{r}
covar <- cov(X)
S <- covar[1:p,1:p]
eig_S <- eigs_sym(S,2)
X_centred <- scale(X,scale=FALSE)
eigen_vectors_S <- eig_S$vectors[,1:2]
Z <- X_centred %*% eigen_vectors_S
```

```{r}

fviz_nbclust(X,kmeans,method="wss",k.max=10)
fviz_nbclust(X,kmeans,method="silhouette",k.max=10)
gap_stat <- clusGap(X,FUN=kmeans,K.max=10,B=100)
fviz_gap_stat(gap_stat,linecolor="steelblue",maxSE=list(method="firstmax",SE.factor = 1))
```
Whit wss we cant see a clear optimal number o cluste. However, average silhouette the best number of clustering is 2 and with gap is 6.For our problem is better to use k=2 because we are classifying our pacients in diabetics or not.We will check both only to see how the results are, but only with partitional algorithm because we doesnt matter to classify our data in 6 groups, only in 2. Now, I will check the two options. We will use means and medoids(PAM and clara). With two cluster , we can see the groups with the PCA
```{r}
color_1 <- "deepskyblue2"
color_2 <- "seagreen2"
color_3 <- "orange2"
color_4 <- "darkorchid4"
color_5 <- "firebrick2"
color_6 <- "darkgreen"

kmeans_X_2 <- kmeans(X,centers=2,iter.max=1000,nstart=100)
colors_kmeans_X_2 <- c(color_1,color_2)[kmeans_X_2$cluster]
plot(Z,pch=19,col=colors_kmeans_X_2,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
The two groups are well differentiated and are very similar to the classification of a person is diabetic or not. Now we check the silhouette.
```{r}
dis = dist(X)

sil_kmeans_X_2 <-silhouette(kmeans_X_2$cluster,dis)
plot(sil_kmeans_X_2, border=NA,col=color_1)
```

The results are good


Then ,With k.medoids, we will use PAM algorithm 
```{r}
pam_X_2 <- pam(X,k=2,metric="manhattan",stand=FALSE)
colors_pam_X_2 <- c(color_1,color_2)[pam_X_2$cluster]
plot(Z,pch=19,col=colors_pam_X_2,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")
```
Its seems that the groups are more mixed than with mean, but it could be wrong because the Pca representation is only a projection. 
```{r}
sil_pam_X_2 <- silhouette(pam_X_2$cluster,dist(X,method="manhattan"))
plot(sil_pam_X_2,col=color_1, border=NA)
```
We can see that the results are slightly worse. Now we use CLARA. Probably we dont have enough data nto feed the algothim
```{r}
clara_X_2 <- clara(X,k=2,metric="manhattan",stand=FALSE)
colors_clara_X_2 <- c(color_1,color_2)[clara_X_2$cluster]
plot(Z,pch=19,col=colors_clara_X_2,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
The groups are very different. There are less green points than in other applications.
```{r}
sil_clara_X_2 <- silhouette(clara_X_2$cluster,dist(X,method="manhattan"))
plot(sil_clara_X_2,col=color_1, border=NA)
```
The average silhouette is lower . 

We continue with 3 folds.
```{r}
kmeans_X_6 <- kmeans(X,centers=6,iter.max=1000,nstart=100)
colors_kmeans_X_6 <- c(color_1,color_2,color_3,color_4,color_5,color_6)[kmeans_X_6$cluster]
plot(Z,pch=19,col=colors_kmeans_X_6,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")
```
```{r}
sil_kmeans_X_6 <- silhouette(kmeans_X_6$cluster,dist(X,"euclidean"))
plot(sil_kmeans_X_6,col=color_1, border=NA)
```

The average silhouette is lower. Its indicates that with 6 cluster the performance is worse. Now we try with PAM.
```{r}
pam_X_6 <- pam(X,k=6,metric="manhattan",stand=FALSE)
colors_pam_X_6 <- c(color_1,color_2,color_3,color_4,color_5,color_6)[pam_X_6$cluster]
plot(Z,pch=19,col=colors_pam_X_6,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
Its seems that is slightly better with PAM. Now we check the silhouttes
```{r}
sil_pam_X_6 <- silhouette(pam_X_6$cluster,dist(X,method="manhattan"))
plot(sil_pam_X_6,col=color_1, border=NA)
```
```{r}
clara_X_6 <- clara(X,k=6,metric="manhattan",stand=FALSE)
colors_clara_X_6 <- c(color_1,color_2,color_3,color_4,color_5,color_6)[clara_X_6$cluster]
plot(Z,pch=19,col=colors_clara_X_6,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
```{r}
sil_clara_X_6 <- silhouette(clara_X_6$cluster,dist(X,method="manhattan"))
plot(sil_clara_X_6,col=color_1, border=NA)

```
For our application, the best partitional clustering is k-means with two clusters.


Now we try hierarchical clustering methods

Agglomerative clustering method is a kind of hierarchical clustering methods in which for each observation we make a cluster and we try to reduce the number of gruop joined gruops. We have four linkage methods, so we try four difference agglomerative clusters.
We start with single linkage, where we compute the distance between a new cluster and all the others with the minimum between the new cluster and each of merge clusters We dont make the dendogam because there are many observations to do that







```{r}
man_dist_X <- daisy(X,metric="manhattan",stand=FALSE)
single_X  <- hclust(man_dist_X,method="single")
cl_single_X <- cutree(single_X,6)
table(cl_single_X)
colors_single_X <- c(color_1,color_2)[cl_single_X]
plot(Z,pch=19,col=colors_single_X,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
The results are awful. Lets we try with Complete linkage
```{r}
complete_X <- hclust(man_dist_X,method="complete")
cl_complete_X <- cutree(complete_X,2)
table(cl_complete_X)
colors_complete_X <- c(color_1,color_5)[cl_complete_X]
plot(Z,pch=19,col=colors_complete_X,main="First two PCs for the diabetes cells data set",xlab="First PC",ylab="Second PC")
```
The results are also awfull.Then we try Average linkage:
```{r}
average_X <- hclust(man_dist_X,method="average")
cl_average_X <- cutree(average_X,2)
table(cl_average_X)
colors_average_X <- c(color_1,color_5)[cl_average_X]
plot(Z,pch=19,col=colors_average_X,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")
```
The results are still very bad. Finally we try ward linkage
```{r}
ward_X <- hclust(man_dist_X,method="ward")
cl_ward_X <- cutree(ward_X,2)
table(cl_ward_X)
colors_ward_X <- c(color_1,color_2)[cl_ward_X]
plot(Z,pch=19,col=colors_ward_X,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
This is the best  agglomerative hierarchical clustering algorithm by far. The shilouttes are
```{r}
sil_ward_X <- silhouette(cl_ward_X,man_dist_X)
plot(sil_ward_X,col=color_1, border=NA)
```
The shiloutte average is bigger than CLARA, but it also smaller than PAM and kmeans.

Now we will work with divisive algorithms in qhich we initally have all observations in one cluster and then and each step divide into two parts the cluster. For that, we will use the most popular algorithm, DIANA.
```{r}
diana_X <- diana(X,metric="manhattan")
cl_diana_X <- cutree(diana_X,2)
table(cl_diana_X)
colors_diana_X <- c(color_1,color_2)[cl_diana_X]
plot(Z,pch=19,col=colors_diana_X,main="First two PCs for the diabetes data set",xlab="First PC",ylab="Second PC")

```
Its seems a goog perfomace so we wiil check the silhouette
```{r}
sil_diana_X <- silhouette(cl_diana_X,man_dist_X)
plot(sil_diana_X,col=color_1,border=NA)
```
We have the best average silhouette with kmeans. Finally, we will check the performance of model-bases clustering methods.First of all, we will check the best assumption of the Mclust

```{r}
BIC_X <- mclustBIC(Z,G=1:5)
BIC_X
plot(BIC_X)

```
The best performace are with three clusters and VVV.Nonetheless, we want to classify in two groups and the best methods with that is VVE(ellipsoidal, equal orientation)
```{r}
BIC_X <- mclustBIC(Z,G=1:2)
Mclust_X <- Mclust(Z,x=BIC_X)
summary(Mclust_X)
plot(Mclust_X,what="classification")


```
Its seem worse than hieretical and partitional clustering
The estimed densities are:
```{r}
plot(Mclust_X,what="density")

```
Now we play the estimated probabilities
```{r}
colors_Mclust_X <- c(color_1,color_2)[Mclust_X$classification]
par(mfrow=c(2,1))
plot(1:n,Mclust_X$z[,1],pch=19,col=colors_Mclust_X,main="Cluster 1",xlab="Gene",ylab="Probability of cluster 1")
plot(1:n,Mclust_X$z[,2],pch=19,col=colors_Mclust_X,main="Cluster 2",xlab="Gene",ylab="Probability of cluster 2")
```
Finally we plot tge points with uncertain
```{r}
par(mfrow=c(1,1))
plot(Mclust_X,what="uncertainty")
```
THere are many ponits with uncertain so is better not to use this tools for our problem.


### Factor Analysis

The aim of the factor analysis is to explain the outcome of our variables from the matrix using fewer variables, that we will call "factors". Ideally, all the information of the women from our dataset can be explained by a small number of factors. We will interpret this factors as latent (unobserver) shared characteristics of the observed data.

Now, let´s focus on finding the latent variables in our data (that is, the variables that we can not meassure), their relationship, their relationship with the variables we indeed can observe, and the underlying structure of the variables we can meassure.
In order to do this, we will use three methods: first we will apply the principal component factor analysis, then we will refine it a bit and apply the principal factor analysis, and finally we will apply maximum likelihood estimation to check if we get a similar result.

Let´s clarify that, as with PCA and Cluster Analysism, we haven´t found two groups that locate very far away one from the other (although with a specific techique we found the clusters), we will perform the factor analysis in the whole dataset, and not in each group separately.

#### Principal Component Factor Analysis

```{r}
library("psych")
```

Let´s first go back to the correlation plot of our observable variables.

```{r}
Y <- scale(X)
corrplot(cor(Y))
corrplot(cor(Y),order="hclust")
```

As we have seen, there are some variables that hold correlations. This is interesting, because now that we will try to find the factors that describe our dataset, we have to see how these variables will be grouped. so the fact that there are some groups of variables that are correlated may suggest us that there may be a factor structure, let´s check if this happens or not.

We will work with our principal component analysis for this part. We said that through three different methods, we would finally stick with 3 principal components.

```{r}
r <- 3
```

Let´s first estimate the matrix M (the loading matrix) and use the varimax rotation, as seen in class.
The interpretation of the loadings would be very simple if the variables could be split into disjoint sets, eah being associated with one factor. A very well known analytical algorithm to rotate the loadings is given by the varimax rotation method. The idea of this method is to find the angle that maximizes the sum of variances of the squred loadings within each column of the matrix.

```{r}
Y_pcs <- prcomp(Y)
M_pcfa <- Y_pcs$rotation[,1:r] %*% diag(Y_pcs$sdev[1:r])
M_pcfa
```

```{r}
M_pcfa <- varimax(M_pcfa)
M_pcfa <- loadings(M_pcfa)[1:p,1:r]
M_pcfa
```
As we can see here, we have three factors, each with different weights from each variable. Let´s plot each factor, with the weights of the variable to see more clearly what charateristic of the population is each factor expressing.

In contrast with the PCA, where the most important component is the one that maximizes the projected variance, here the most important factor in the analysis is the one that (after rotation) gives the maximal interpretation. 

#### Factor 1

```{r}
plot(1:p,M_pcfa[,1],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the first factor")
abline(h=0)
text(1:p,M_pcfa[,1],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
As we can see in the plot, there are some variables close to zero, but there are others that have a "big" negative weight on the factor. These two variables are "skin thickness" and "BMI". This factor may be an (inverse) index of the patients "external signs", characteristics that can be seen by a doctor by just looking at the patient.

#### Second Factor

```{r}
plot(1:p,M_pcfa[,2],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the second factor")
abline(h=0)
text(1:p,M_pcfa[,2],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
Our second factor is very interesting, and seems to be explained spetially by variables like "pregnancies", "age" and have obviosuly more to do with the life cycle pr the chronology of the patient. It is obvious that one big characteristic of the data is the moment of the life of the women, and this factor is showing this hidden characteristic.

#### Factor 3

```{r}
plot(1:p,M_pcfa[,3],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the third factor")
abline(h=0)
text(1:p,M_pcfa[,3],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
Our third factor seems to be showing variables with positive and high weights such as "glucose", "insulin" and basically is showing us an index of of metabolism characteristics, related to the sugar in blood and the hormones that the person produces.

This first approach to factor analysis brought some very interesting results. What we can say is that there are three fundamentl factors in our data: one describing more external characteristics of the person (the inverse of a healthy looking person), second factor is showing characteristics related to life cycle and the moment in which the woman is in her life, and third factor is showing an index of metabolism (sugar and hormonal values).

Let´s now check the covariance matrix of the error.

```{r}
Sigma_nu_pcfa <- diag(diag(cov(Y) - M_pcfa %*% t(M_pcfa)))
Sigma_nu_pcfa
```
The matrix of erros will be later used to estimate the scores of the factors.

Let´s check now the communalities and uniqueness. First, we calculate the communalities and plot them with the variables to see what variables better explain the factors.

```{r}
comm_pcfa <- diag(M_pcfa %*% t(M_pcfa))
comm_pcfa
```

```{r}
plot(1:p,sort(comm_pcfa,decreasing=TRUE),pch=20,col=color_1,xlim=c(0,34),xlab="Variables",ylab="Communalities",
     main="Communalities with PCFA")
text(1:p,sort(comm_pcfa,decreasing=TRUE),labels=names(sort(comm_pcfa,decreasing=TRUE)),pos=4,col=color_5,cex=0.75)
```
The variables that are better explained by the factors are BMI, SkinThickness, Glucose, Insuine, Age and Pregnancies. These characteristics are the ones that have a bigger weight on the communality part, and not in the uniqueness part.
Of course, when we plot the uniqueness and the variables, we get the opposite plot, as follows:


```{r}
uniq_pcfa <- 1 - comm_pcfa
uniq_pcfa
names(uniq_pcfa) <- names(comm_pcfa)
uniq_pcfa
```

```{r}
plot(1:p,sort(uniq_pcfa,decreasing=TRUE),pch=20,col=color_1,xlim=c(0,34),xlab="Variables",ylab="Uniquenesses",
     main="Uniquenesses with PCFA")
text(1:p,sort(uniq_pcfa,decreasing=TRUE),labels=names(sort(uniq_pcfa,decreasing=TRUE)),pos=4,col=color_5,cex=0.75)
```
As we can see, the diabetes pedigree function is the variables the less explaines the factors, followed by blood pressure.

Let´s estimate the factors scores now by utilizing the estimation of matrix M, the estimation of the correlation matrix of the errors and the scaled dataset (we have to define it as a matrix).

```{r}
Y <- as.matrix(Y)
F_pcfa <- Y %*% solve(Sigma_nu_pcfa) %*% M_pcfa %*% solve(t(M_pcfa) %*% solve(Sigma_nu_pcfa) %*% M_pcfa)

colnames(F_pcfa) <- c("Factor 1","Factor 2","Factor 3")
```

Now, we can plot correlation matrix of the factor scores and a simple scatterplot of the 3 factors, to check if there are any relationships between them.

```{r}
pairs(F_pcfa,pch=19,col=color_1)
corrplot(cor(F_pcfa),order="hclust")
```
As we can see, the factors are for the most part uncorrelated. To check how well the model is working, let´s estimate the residuals and see the correlation matrix of them.

```{r}
Nu_pcfa <- Y- F_pcfa %*% t(M_pcfa)
corrplot(cor(Nu_pcfa),order="hclust")
```
As we can see, there are some correlations between the residuals. This is not a good sign, as it may be telling us that there is a lot in our model that is not being explained.

Let´s study what happens when we do Principal Factor Analysis.

#### Principal Factor Analysis

In order to do this, we will first have to estimate the correlation matrix of X.

```{r}
R_X <- cor(X)
```

After doing this, we have to get sigma, its eigenvalues and eigenvectors, as follows:
```{r}
MM <- R_X - Sigma_nu_pcfa
MM_eig <- eigen(MM)
MM_values <- MM_eig$values
MM_vectors <- MM_eig$vectors
```

We can now estimate the matrix M and use the varimax criterion. Again, we will keep r=3, as this is the result we got from different testing methods in the previous part of the research.

```{r}
M_pfa <- MM_eig$vectors[,1:r] %*% diag(MM_eig$values[1:r])^(1/2)
M_pfa <- varimax(M_pfa)
M_pfa <- loadings(M_pfa)[1:p,1:r]
M_pfa
```
Now, we can compare the results we get through this method and the one we got we the previous method.

For the first factor, we can see there is not a lot of linearity.

```{r}
par(mfrow=c(2,3))
plot(M_pcfa[,1],M_pfa[,1],pch=19,col=color_1,main="First factors with PCFA and PFA",xlab="PCFA",ylab="PFA")
```
For the second factor, there is also not so much linearity.

```{r}
par(mfrow=c(2,3))
plot(M_pcfa[,2],M_pfa[,2],pch=19,col=color_1,main="Second factors with PCFA and PFA",xlab="PCFA",ylab="PFA")
```
For the third factor, there is also not so much linearity.
```{r}
par(mfrow=c(2,3))
plot(M_pcfa[,3],M_pfa[,3],pch=19,col=color_1,main="Third factors with PCFA and PFA",xlab="PCFA",ylab="PFA")
```
Our three factors don´t seem to be equal through both methods, but look quite alike. Let´s see what happens when we check the weights.


```{r}
plot(1:p,M_pfa[,1],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the first factor")
abline(h=0)
text(1:p,M_pfa[,1],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
```{r}
plot(1:p,M_pfa[,2],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the second factor")
abline(h=0)
text(1:p,M_pfa[,2],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
```{r}
plot(1:p,M_pfa[,3],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the third factor")
abline(h=0)
text(1:p,M_pfa[,3],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
The three factors look very similar with one exception, the third one is taking into account the inverse of what the third factor in the other technique was taking into account. So here, the factors is describing the metabolism of the woman but in terms of the inverse.

Let´s estimate the covariance matrix of the errors and comparen them with the one obtained in the previous model.

```{r}
Sigma_nu_pfa <- diag(diag(R_X - M_pfa %*% t(M_pfa)))
par(mfrow=c(1,1))
plot(diag(Sigma_nu_pcfa),diag(Sigma_nu_pfa),pch=19,col=color_1,main="Noise variances with PCFA and PFA",
     xlab="PCFA",ylab="PFA")
```
We can see that there are no big differences between our two techniques of finding the factors underlying our dataset (the main characteristics of the indian women taken into consideration).
Let´s now check the communalities and uniqueness and compare them with the ones we got in the previous technique:

```{r}
comm_pfa <- diag(M_pfa %*% t(M_pfa))
names(comm_pfa) <- colnames(Y)
```

First, communalities obtained in the Principal Factor Analysis:

```{r}
sort(comm_pfa,decreasing=TRUE)
```
Now, communalities obtained in the Principal Component Factor Analysis:

```{r}
sort(comm_pcfa,decreasing=TRUE)
```
As we can see the communalities in the Principal Component Factor Analysis are slightly bigger than in our Principal Factor Analysis. Let´s compare the uniqueness we get from both techniques.

First, the uniquensses in our Principal Factor Analysis:

```{r}
uniq_pfa <- diag(Sigma_nu_pfa)
names(uniq_pfa) <- names(comm_pfa)
sort(uniq_pfa,decreasing=TRUE)
```
Noe, the uniquenesses in our Principal Component Factor Analysis:

```{r}
sort(uniq_pcfa,decreasing=TRUE)
```
As we can see, the uniqueness tends to increase in the Principal Factor Analysis in comparison with the Principal Component Factor Analysis.

Let´s plot the variables that better explain the factors.

```{r}
plot(1:p,sort(comm_pfa,decreasing=TRUE),pch=20,col=color_1,xlim=c(0,34),xlab="Variables",ylab="Communalities",
     main="Communalities with PFA")
text(1:p,sort(comm_pfa,decreasing=TRUE),labels=names(sort(comm_pfa,decreasing=TRUE)),pos=4,col=color_5,cex=0.75)
```
As we can see, they are: BMI, Insulin, Skin Thickness, Age and Pregnancies.
Let´s plot the uniquenesses and get the opposite result:

```{r}
plot(1:p,sort(uniq_pfa,decreasing=TRUE),pch=20,col=color_1,xlim=c(0,34),xlab="Variables",ylab="Uniquenesses",
     main="Uniquenesses with PFA")
text(1:p,sort(uniq_pfa,decreasing=TRUE),labels=names(sort(uniq_pfa,decreasing=TRUE)),pos=4,col=color_5,cex=0.75)
```
As we can see, the variables that less explain the factors are the Diabetes Pedigree Function and the Blood Preassure.

Now, we can estimate the scores of the factors we got with this method.
```{r}
F_pfa <- Y %*% solve(Sigma_nu_pfa) %*% M_pfa %*% solve(t(M_pfa) %*% solve(Sigma_nu_pfa) %*% M_pfa)
colnames(F_pfa) <- c("Factor 1","Factor 2","Factor 3")
```

We can plot the scatterplot and a correlation matrix and check if the factors in Principal Factor Analysis are or not correlated.

```{r}
pairs(F_pfa,pch=19,col=color_1)
corrplot(cor(F_pfa),order="hclust")
```
There is no correlation between the factors. Let´s chech now a correlation matrix between the estimates of the Principal Component Factor Analysis and the Principal Factor Analysis:

```{r}
cor(F_pcfa,F_pfa)
corrplot(cor(F_pcfa,F_pfa))
```
Both ways of estimating the factors seem very similar. As we have seen, the factors are perfectly correlated, and the third one has a perfect inverse correlation, as we expressed before when we commented that it was expressing the opposite index in comparison with the technique used before.

Finally, let´s plot the correlation matrix between the residuals, and check how much variability we can not explain with our model.

```{r}
Nu_pfa <- Y - F_pfa %*% t(M_pfa)
corrplot(cor(Nu_pfa),order="hclust")
```
As we can see, there still serious correlations between the residuals, showing us that there is still some vairability that we can not explain with the model. 

#### Maximum Likelihood Estimation

As now we are assuming our data comes from a gaussian distribution (and indeed, it had been transformed into normality in the first part of the project) we can perform likelihood ratio test to say how many factors are we going to use.
By doing this test, we will be collecting evidence to reject (or not) our null hypotesis of a reduced model, against an alternative hypotesis of a more complete model. For example, first we will try if 1 factor is enough (H0) ir not (H1), and so on.

```{r}
Y_mle_1 <- factanal(Y,factors=1,rotation="varimax",scores="Bartlett")
Y_mle_1$STATISTIC
Y_mle_1$PVAL
```
We reject the null hypotesis, as our p-value is very small. Let´s go to the next one, with two factors:

```{r}
Y_mle_2 <- factanal(Y,factors=2,rotation="varimax",scores="Bartlett")
Y_mle_2$STATISTIC
Y_mle_2$PVAL
```
Our p-value is very small, so we reject again the null hypotesis.

```{r}
Y_mle_3 <- factanal(Y,factors=3,rotation="varimax",scores="Bartlett")
Y_mle_3$STATISTIC
Y_mle_3$PVAL
```
Now, we can reject the null hypotesis saying that our model with three factors is not enough (as p-value is larger than 0.05). This way, we can say that maybe three factors are not the right number, confirming that the way in which we were working maybe was not alright.

```{r}
Y_mle_4 <- factanal(Y,factors=4,rotation="varimax",scores="Bartlett")
Y_mle_4$STATISTIC
Y_mle_4$PVAL
```
As we see, we keep on rejecting the null hypotesis. This may be due to the fact that our data does not really behave as propper gaussian distribution. We will still work with three factors, as we only have 8 variables, 3 seems like a reasonable number.


Let´s check the loading matrix (the one that multiplies the factor matrix in our model, and is a matrix of constants).
```{r}
M_mle <- loadings(Y_mle_3)[1:p,1:r]
M_mle
```
We can compare with the estimations we got from the PFA.

```{r}
par(mfrow=c(2,3))
plot(M_pfa[,1],M_mle[,1],pch=19,col="deepskyblue2",main="First factors with PFA and MLE",xlab="PFA",ylab="MLE")
```
```{r}
plot(M_pfa[,2],M_mle[,2],pch=19,col="deepskyblue2",main="Second factors with PFA and MLE",xlab="PFA",ylab="MLE")
```

```{r}
plot(M_pfa[,3],M_mle[,3],pch=19,col="deepskyblue2",main="Third factor with PFA and fifth factor with MLE",xlab="PFA",ylab="MLE")
```

The estimates of the loading matrix don´t seem to be very similar, at least not at first view. So we will plot the weights to see what variables are explaining each factor.

```{r}
plot(1:p,M_mle[,1],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the first factor")
abline(h=0)
text(1:p,M_mle[,1],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```

```{r}
plot(1:p,M_mle[,2],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the second factor")
abline(h=0)
text(1:p,M_mle[,2],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```

```{r}
plot(1:p,M_mle[,3],pch=19,col=color_1,xlab="",ylab="Weights",main="Weights for the third factor")
abline(h=0)
text(1:p,M_mle[,3],labels=colnames(X),pos=1,col=color_5,cex=0.75)
```
Finally, we see that the weights assure us that the variables that explain each factor are the same in this case as in the other two before (PFA and PCFA). Only in the case of the first and the third factors, they are inverted.

Let´s check the covariance matrix of the errors now and compare them with the ones that we got in our Principal Factor Analysis:

```{r}
Sigma_nu_mle <- diag(diag(cov(Y) - M_mle %*% t(M_mle)))
par(mfrow=c(1,1))
plot(diag(Sigma_nu_pfa),diag(Sigma_nu_mle),pch=19,col=color_1,main="Noise variances with PFA and MLE",
     xlab="PFA",ylab="MLE")
```
As we can see, there are some differences between the noise in both models.
Let´s check on the comunnalities and uniquenesses.

First, let´s compare the communalities between our Maximum Likelihood estimation and the one obtained by our Principal Factor Analysis:

```{r}
comm_mle <- diag(M_mle %*% t(M_mle))
names(comm_mle) <- colnames(Y)
sort(comm_mle,decreasing=TRUE)
```

```{r}
sort(comm_pfa,decreasing=TRUE)
```
As we can see, the communalities in the MLE are highers than the one obtained with PFA in many cases: BMI, Age, Glucose while not in others such as: pregnancies, insulin, blooad preassure, diabetes pedigree function and skinthickness.

Let´s check what happens with the uniquensses, although we should expect to be the opposite as what we just saw, as communalities + uniquensses sum 1.

Uniquensses in the MLE:

```{r}
uniq_mle <- diag(Sigma_nu_mle)
names(uniq_mle) <- names(comm_mle)
sort(uniq_mle,decreasing=TRUE)
```
Uniquenesses in the PFA:

```{r}
sort(uniq_pfa,decreasing=TRUE)
```
The variables that are best explained by the factors in both cases are the BMI, Age, Glucose, insulin. While the worst explained by the factors are: diabetespedigreefunction, blood preassure.

Let´s estimate the factors scores and check the correlation:
```{r}
F_mle <- Y %*% solve(Sigma_nu_mle) %*% M_mle %*% solve(t(M_mle) %*% solve(Sigma_nu_mle) %*% M_mle)
colnames(F_mle) <- c("Factor 1","Factor 2","Factor 3")
pairs(F_mle,pch=19,col="deepskyblue2")
corrplot(cor(F_mle),order="hclust")
```

There are some small correlations between some factors, but very small.
Let´s now check the correlation matrix between the PFA and MLE estimations.

```{r}
cor(F_pfa,F_mle)
corrplot(cor(F_pfa,F_mle))
```
Again, we see some very small (almost null) correlation between different factors.

Let´s estimate the residuals and check the correlation matrix:

```{r}
Nu_mle <- Y - F_mle %*% t(M_mle)
corrplot(cor(Nu_mle),order="hclust")
```
Again, we see that there is some correlation between the residuals of some variables. Variability that our model can not account for.

Let´s the the correlation matrix between this residuals and the ones obtained in Princial Factor Analysis.
```{r}
corrplot(cor(Nu_pfa,Nu_mle))
```
As always before, there is correlation between the residuals of some variables that our model can not explain.

### Multidimensional Scaling

First, I will create a matrix of the similarities between the different political parties in Uruguay. Let us first introduce what each party name refers to:

- PN ("Partido Nacional): one of the two historical most classical parties of Uruguay (existing since independance). It is characterized as conservador and neoliberal, specially related to church.
-PC ("Partido Colorado"): the other historical most classical party of Uruguay (also existing since independance). These two parties were the only ones existing in Uruguay until the seventies. This is also conservator party.
- FA ("Frente Amplio"): this is a left-wing party, including many small left parties (from communist, socialist and parties related to worker unions). 
- PERI ("Partido Ecologista Radical Instransigente"): this is a new party, that is related a bit to the left wing but is specially worried about environmental resources and the use of agrochemicals (being Uruguay an economy based in agriculture).
- PVA ("Partido Verde Animalista"): this is a party related to veganism and is very opposed to all other parties, believing in big conspirataional theories and very against projects related to selling resources to other countries.
- PDLG ("Partido de la Gente"): this is very small partythat follows an entrepreneur as leader who had a lot of success, but mainly believes in the hard work and decreasing taxes.
- CA ("Cabildo Abierto"): is a party of ex military people, spetially worried about security and law enforcement.


Based on my own opinion, here is a simmilarity matrix between the different parties.


```{r}
pol_part <- c(1,0.7,0.1,0.2,0.05,0.6,0.85,0.7,1,0.2,0.7,0.05,0.7,0.8,0.1,0.2,1,0.7,0.5,0.1,0.05,0.2,0.2,0.7,1,0.8,0.1,0.05,0.05,0.05,0.5,0.8,1,0.2,0.1,0.6,0.7,0.1,0.1,0.2,1,0.7,0.85,0.8,0.05,0.05,0.1,0.7,1)
pol_part <- matrix(pol_part,nrow=7,dimnames = list(c("PN","PC","FA","PERI","PVA","PDLG","CA"), c("PN","PC","FA","PERI","PVA","PDLG","CA")))
pol_part
```

Now, I will transform this matrix into a dissimilarity one.

```{r}
library(smacof)
diss_pol_part <- sim2diss(pol_part,method="reverse",to.dist=TRUE)
diss_pol_part
max(diss_pol_part)
min(diss_pol_part)
```
We can see that, for example "Cabildo Abierto" and "Frente Amplio" the most left and right parties have a dissimiliarity equal to 1. "CA" also has a 100% dissimilarity with the ecologist party (PERI) as they really have nothing in common, they are completly one against the other. Meanwhile, the rest have values between 0 and 1.

Let´s now find the first k=6 principal coordinates.

```{r}
mds_parties <- cmdscale(diss_pol_part,k=6,eig=TRUE)
mds_parties
```
As we can see, 5 out of the 6 eigenvalues are negative, so we will just work with these 5. We can now find the precision measures for the 5 possitive eigenvalues.

```{r}
mds.m <- cumsum(mds_parties$eig[1:5]/sum(abs(mds_parties$eig)))
mds.m
```

```{r}
par(mfrow=c(1,2))
plot(1:7,mds_parties$eig,type="b",col=color_1,pch=20,xlab="Eigenvalue number",ylab="Eigenvalue",main="Eigenvalues")
abline(h=0)
plot(1:5,mds.m,type="b",col=color_1,pch=20,xlab="Eigenvalue number",ylab="Precision measure",main="Precision measure for the positive eigenvalues")

```
As we can see, with two eigenvalues we explain more than 75% of the variability of our dataset.
We can plot a perceptual map:

```{r}
par(mfrow=c(1,1))
plot(mds_parties$points[,1],mds_parties$points[,2],xlab="Principal coordinate 1",ylab="Principal coordinate 2",pch=20,col=color_1)
text(mds_parties$points[,1],mds_parties$points[,2],labels=rownames(mds_parties$points),col=color_4,pos=1)
abline(v=0,h=0)
```
As we can see, our first principal coordinate divides clearly between parties that are left wing and the ones that are right wing. This is why PERI, FA, and PVA are on the right side and PC, PN, CA, PDLG on the other side (it´s funny, left and right are exchanged in the plot, but just as a coincidence). 

The second coordinate seems to be creating a separation between the parties that are more focused on the countriside (or have more support there), while others do not. "PC" and "PERI" have in common that they are both concerned about the countryside and are always talking about ways of making the countryside more productive. Meanwhile "PDLG" is very related to the cities and always proposses projects to bring people to the cities from the country, while "PVA" is against animal exploitation (the first industry of the country) so it is very rejected outsude the cities. So we could say that this second coordinate meassures the acceptance in the countryside.




### Correspondance Analysis

The correspondence analysis provides tools for analyzing the associations between rows and columns of contingency tables.So, the main idea of this analysis is to develop simple indices that can show the relatoinships between the row and the columns categories. These indices, will tell us simultaneously which column categories actually have more weight in a row category and vice-versa. We will finally use two factors (indices), to show the result in a two dimensional plot, showing the relationships between the rows and the columns of the table.

First, we will add the data provided by the professor on health.

```{r}
health <- matrix(c(243,789,167,18,6,220,809,164,35,6,147,658,181,41,8,90,469,236,50,16,53,414,306,106,30,44,267,284,98,20,20,136,157,66,17),nrow=7,ncol=5,byrow=TRUE)

row.names(health)<-c("16-24","25-34","35-44","45-54","55-64","65-74","75+")

colnames(health)<-c("VG","G","R","B","VB")

health <- as.table(health)

health
sum(health)
```
As we can see, we have a contingency table of 6371 individuals that are classified between ages (intervals) and their state of health being:

-VG : "very good"
-G: "good"
-R: "Fair"
-B: "Bad"
-VB: "Very Bad"


Let´s plot a baloon plot to see ghrpahically the magnitude of each frequency.

```{r}
library(ggpubr)
health_df <- as.data.frame(health)
ggballoonplot(health_df,fill="value")+scale_fill_viridis_c(option="A")
```
As we can see in the ballonplot, the classes are not similarly distributed in our contingency table. We can see that most of our dataset are young people  (aged between 16-24 and 25-34) whose health is good. What is more, most of our dataset is composed by people felling good while a minority has a very bad health (specially when young).
We can say that there are more young people than old people in our dataset.

We can do a joint barplot with the two variables to check on the distribution of our data.

```{r}
plot(health,xlab="Age",ylab="Health Status",col=color_1,main="Joint barplot")

```
This plot can help us check if there is any kind of relationship between the two variables. We can see from this joint barplot that for people that have a good health the width of their bars change with the age but not so much. Specially with the people that have a very bad health, we see that the classes look very homogeneous. In the people that have a bad health the classes look homogeneous until the people start getting older, so in the larger ages the bars are bigger. All in all, we can say that as we check people who have better health, the classes become more homogeneous, in some way. Although the relationship is not extremely clear we can say that for sure the two variables are somehow related. This is something trivial: we have every reason to believe that "age" and "health" are dependant variables, as it is to expect that health deteriorates while age increases.

The intention now is to find quantitative variables that represent the classes obtained by the qualitative variables here described. To do so, first we have to determine if both qualitative variables are independent or not. This is because the whole intention is to understand the relationship between two qualitative variables, but if we prove they are independendant, there we do not have to follow with the exercise.

Even though common sense would say that these vairables have a dependance relationship, let´s study it in depth.

This way, we will test the independency of the two qualitative variables by contrasting the null hypotesis ("there is no independance between the variables") and the alternative hypotesis ("there exists some dependency, unknown, between the variables"). We will, then, measure the divergence between the correspondence table and the one we should expect under the null hypotesis. In other words, we will compare the relative frequencies (total frequencies divided by the total number of observations) and the ones we would obtain if we would consider there ir independence between the variables (we would multiply the relative frequencies) and then we will use a Chi-square statistic (this is how the distribution behaves under H0) to see if we reject or not the null hypotesis. 

First, let´s produce our relative contingency table, and check that the sum of coumns and rows is 1.

```{r}
P <- prop.table(health)
addmargins(P)
```
```{r}
health_ct <- chisq.test(health)
health_ct
```
As our p-value is lower than 0.01 we can say that at a confidence level of 99% we can have enough evidence to reject the null hypotesis of independence and believe there may be some kind of dependency between our observable variables. There is clearly a significance dependance between the two variables.

Let´s do correspondance analysis now.

Our objective is to first define the distances between the classes (separately) and then get a configuration of points such that their distances is similar to the one between classes.

In order to first define the distnces between classes, we have to get rid of the effect that will come up for the fact that each row and column have very different sums. As we saw before, the classes are unbalances: most of the people in our dataset are young, for instance. In order to do so, we will do a kind of normalization of the relative frequency matrix, getting what is called a matrix of row profiles, dividing each element by the total sum of the row. Then we will use the Chi squared distance, using a distance that resembles a lot to the Mahalanobich distance. This way we perform kind of a double standarization. We then do the same with a matrix of columns profiles.  

```{r}
trf_r <- rowSums(P)
trf_r

trf_c <- colSums(P)
trf_c
```

We can calculate the Row and columns profiles
```{r}
D_r <- diag(trf_r)
D_r
D_c <- diag(trf_c)
D_c
```

We can now compute the matrices of row and columns profiles

```{r}
P_r <- solve(D_r) %*% P
P_r
apply(P_r,1,sum)

P_c <- solve(D_c) %*% t(P)
P_c
apply(P_c,1,sum)
```


With these matrixes, we will create a matrix M, that will be the result of substracting the expected and observed relative frequencies (under the independance hypotesis), standarized by the relative frequencies corresponding to the rows and the one to the columns (first and second variables). 

Then we do a singular value decomposition of this matrix. 

```{r}

M <- diag(1/sqrt(trf_r)) %*% (P - trf_r %*% t(trf_c)) %*% diag(1/sqrt(trf_c))
M
M_svd <- svd(M)
M_svd

# Define matrix Lambda, Gamma and Theta

Lambda_M <- diag(M_svd$d)
Gamma_M <- M_svd$u
Theta_M <- M_svd$v
```

With this decomposition, we can find both matrixes X_r and X_c (for first and second variable). As both matrixes have the same amount of columns, I can plot them in the same space. We will plot just two of them (two columns of the two matrixes). When we do this, it is important to check how much vairability is explained between the two dimensions together. 

```{r}
X_r <- diag(1/sqrt(trf_r)) %*% Gamma_M[,1:2] %*% Lambda_M[1:2,1:2]
X_r

X_c <- diag(1/sqrt(trf_c)) %*% Theta_M[,1:2] %*% Lambda_M[1:2,1:2]
X_c
```

We can plot just two of them (two columns of the two matrixes). When we do this, it is important to check how much vairability is explained between the two dimensions together. We can also use the package "ca" (easier and straightforward) to do the correspondance analysis, and this way we can avoid all the preovious coding, and just plot the results.

```{r}
library(ca)

ca_health <- ca(health) 
ca_health
plot(ca_health)
```
What can we conclude from this plot?


First of all, with dimension one we can explain 97.3% of the variability of our data. With the two together we can explain 98.8% of the total variability of the data. This is an outstanding result. This means that the summary is not leaving out information about the data. 

Second of all, from the structure we can see of the plot, it is obvious there is a clear relationship between some rows and some columns. Let´s see what are the characteristics of it:

1- In general the proximity of two rows (two columns) indicates a similar profile in these two rows (columns). By profile I reffer to the conditional frequency distribution of the row (or column). In other words, rows 16-24 and 25-34 are almost proportional, they have very similar profile.

2- in general, the proximity of a particular row to a particular column indicates that one has an important weight in the other. On the contrary, a row that is very distant from a particlar column may indicate that there almost no observations in this column for this row. From this we can say:

i) The health category "very good" is very related to the two first intervals of age, that is the youngest people. This is coherent with our first analysis.
ii) Good health is specially associated with the category of people between 35-44 years old.
iii) Label "fair" has a big weight (is very related) to the interval 55-64 years old.
vi) People feeling very bad are specially old people.

3 - In general, the origin is the average of the factors. This is why a label projected close to the origin indicates an avarage profile. In particular, people between 45-54 years old are not so distinct in the different helth categories, relatively speaking.


4- In general, labels that are in the opposite side of the origin tend to be negatively associated one to the other. As categories "bad" and "very bad" are on opposite sides of the origine to categories describing young people, we can say that these lables are probably negatively associated (young people and bad health).












